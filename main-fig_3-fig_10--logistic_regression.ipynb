{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5f3209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.colors import LogNorm\n",
    "from config import *\n",
    "\n",
    "x_min, x_max = 0.2, 3.5\n",
    "d = 200\n",
    "results = [{**a, 'data_input_dim': d} for a in pickle.load(open(f\"neurips_code/results_linear_with_mse_d={d}.pkl\", \"rb\"))]\n",
    "d = 1600\n",
    "results += [{**a, 'data_input_dim': d} for a in pickle.load(open(f\"neurips_code/results_linear_with_mse_d={d}.pkl\", \"rb\"))]\n",
    "d = 1600\n",
    "results += [{**a, 'data_input_dim': d} for a in pickle.load(open(f\"neurips_code/results_linear_with_mse_d={d}_v2.pkl\", \"rb\"))]\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df['student_train_acc'] = df['train_acc']\n",
    "df['student_test_acc'] = df['test_acc']\n",
    "df_orig = df.copy()\n",
    "\n",
    "df = df[df.student_shuffle_input_intra_class == False]\n",
    "df = df.groupby(['student_train_frac','data_num_samples_per_class','data_input_dim']).mean(numeric_only=True).reset_index()\n",
    "b = df[df.data_input_dim == d]\n",
    "\n",
    "\n",
    "def to_grid(z,a,d):\n",
    "    table = a.pivot(index='student_train_frac', columns='data_num_samples_per_class', values=z)\n",
    "    X = table.columns.values * 2 / d\n",
    "    Y = table.index.values\n",
    "    Z = table.values\n",
    "    return X, Y, Z\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(10, 6),sharex=True)\n",
    "axes = axes.flatten()\n",
    "metrics = ['teacher_train_acc','student_train_acc', 'student_test_acc', 'match_teacher_test_acc',  'train_mse','test_mse',]\n",
    "\n",
    "for ax, metric in zip(axes, metrics):\n",
    "    X, Y, Z = to_grid(metric,b,d)\n",
    "    if metric == 'test_mse' or metric == 'train_mse':\n",
    "        pcm = ax.pcolor(X, Y, Z, cmap='Greens_r', norm=LogNorm(vmin=Z.min(), vmax=Z.max()), shading='auto')\n",
    "        fig.colorbar(pcm, ax=ax)\n",
    "    elif metric == 'teacher_train_acc':\n",
    "        a = b.groupby('data_num_samples_per_class')['train_acc'].mean().reset_index()\n",
    "        ax.plot(a.data_num_samples_per_class  * 2 / d, a.train_acc, c='black')\n",
    "    else:\n",
    "        pcm = ax.pcolor(X, Y, Z, cmap='Spectral', vmin=0, vmax=1, shading='auto')\n",
    "        fig.colorbar(pcm, ax=ax)\n",
    "    ax.set_xlabel(r'$\\alpha$')\n",
    "    ax.set_ylabel(r'$\\rho$')\n",
    "    ax.set_title(metric_styles[metric]['label'])\n",
    "for ax in axes:\n",
    "    ax.set_xlim(x_min,x_max)\n",
    "axes[0].set_ylabel('accuracy')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURE_DIR / f'logistic_regression_phase_diag_linear_with_mse_d={d}.pdf',bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51504e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df[df.data_input_dim == 1600].groupby(['data_num_samples_per_class']).mean(numeric_only=True).reset_index()\n",
    "alpha_t_label_min = a[a.teacher_train_acc >= 0.99].data_num_samples_per_class.max() * 2 / d\n",
    "alpha_t_label_max = a[a.teacher_train_acc < 0.99].data_num_samples_per_class.min() * 2 / d\n",
    "alpha_t_label= (alpha_t_label_min + alpha_t_label_max) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0452d0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df[np.logical_and(df.data_input_dim == 1600,np.isclose(df.student_train_frac,0.80306122))].groupby(['data_num_samples_per_class']).mean(numeric_only=True).reset_index()\n",
    "alpha_s_label_min = a[a.student_train_acc >= 0.99].data_num_samples_per_class.max() * 2 / d\n",
    "alpha_s_label_max = a[a.student_train_acc < 0.99].data_num_samples_per_class.min() * 2 / d\n",
    "alpha_s_label= (alpha_s_label_min + alpha_s_label_max) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311045fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df[np.logical_and(df.data_input_dim == 1600,np.isclose(df.student_train_frac,0.80306122))].groupby(['data_num_samples_per_class']).mean(numeric_only=True).reset_index()\n",
    "alpha_s_id = a[a.student_test_acc >= 0.99].data_num_samples_per_class.min() * 2 / d\n",
    "\n",
    "print(alpha_t_label, alpha_s_label, alpha_s_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c198f814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "\n",
    "class Phases(IntEnum):\n",
    "    NADA = 4\n",
    "    GENERALIZATION = 0\n",
    "    GEN_LEAKAGE = 1\n",
    "    MEM_FAIL_DONT_LEARN_TEACHER = 2\n",
    "    MEM_FAIL_LEARN_TEACHER = 3\n",
    "# Phase classification function\n",
    "def get_phase(train_acc, test_acc, match_teacher_test_acc, train_mse, test_mse):\n",
    "    if train_acc >= 0.99:\n",
    "        if test_acc >= 0.99:\n",
    "            return Phases.GENERALIZATION\n",
    "        else:\n",
    "            if test_acc >= 0.55:\n",
    "                return Phases.GEN_LEAKAGE\n",
    "            else:\n",
    "                return Phases.NADA\n",
    "    else:\n",
    "        if match_teacher_test_acc >= 0.99:\n",
    "            return Phases.MEM_FAIL_LEARN_TEACHER\n",
    "        else:\n",
    "            return Phases.MEM_FAIL_DONT_LEARN_TEACHER\n",
    "\n",
    "a = df[df.data_input_dim==d].groupby(['student_train_frac', 'data_num_samples_per_class']).mean(numeric_only=True).reset_index()\n",
    "# Apply the phase classification\n",
    "a['phase'] = a.apply(lambda row: get_phase(\n",
    "    row['train_acc'],\n",
    "    row['test_acc'],\n",
    "    row['match_teacher_test_acc'],\n",
    "    row['train_mse'],\n",
    "    row['test_mse']\n",
    "), axis=1)\n",
    "\n",
    "# Compute alpha and rho\n",
    "a['alpha'] = a['data_num_samples_per_class'] * 2 / d  # assuming d = 1000, 2 classes\n",
    "a['rho'] = a['student_train_frac']\n",
    "\n",
    "#a = pd.concat([a, other_a], axis=0)\n",
    "\n",
    "# Define colormap for phases\n",
    "phase_colors = {\n",
    "    Phases.GENERALIZATION: \"#2ca02c\",             # green\n",
    "    Phases.GEN_LEAKAGE: \"#1f77b4\",                # blue\n",
    "    Phases.MEM_FAIL_DONT_LEARN_TEACHER: \"#d62728\", # red\n",
    "    Phases.MEM_FAIL_LEARN_TEACHER: \"#ff7f0e\",      # orange\n",
    "}\n",
    "\n",
    "phase_labels = {\n",
    "    Phases.GENERALIZATION: \"Generalization\",\n",
    "    Phases.GEN_LEAKAGE: \"Gen Leakage\",\n",
    "    Phases.MEM_FAIL_DONT_LEARN_TEACHER: \"Mem Fails (Learn Teacher)\",\n",
    "    Phases.MEM_FAIL_LEARN_TEACHER: \"Mem Fails\",\n",
    "}\n",
    "\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "\n",
    "# Create 2D grid of phases\n",
    "phase_grid = a.pivot(index='rho', columns='alpha', values='phase')\n",
    "X = phase_grid.columns.values\n",
    "Y = phase_grid.index.values\n",
    "Z = phase_grid.values\n",
    "\n",
    "# Define colormap\n",
    "cmap = ListedColormap([\n",
    "    '#2ca02c',   # GENERALIZATION (green)\n",
    "    '#1f77b4',   # GEN_LEAKAGE (blue)\n",
    "    '#d62728',   # MEM_FAIL_DONT_LEARN_TEACHER (red)\n",
    "    '#ff7f0e',   # MEM_FAIL_LEARN_TEACHER (orange)\n",
    "    '#7f7f7f',   # NADA (grey)\n",
    "])\n",
    "bounds = [0, 1, 2, 3, 4, 5]\n",
    "norm = BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1,4,figsize=(22/2*0.8+22/2*0.8/3, 4*2*0.8/2))\n",
    "ax = axes[-1]\n",
    "pcm = ax.pcolor(X, Y, Z, cmap=cmap, norm=norm, shading='auto')\n",
    "ax.axvline(alpha_t_label,color=alpha_t_label_color, linestyle='--')\n",
    "ax.set_xlabel(r'$\\alpha$')\n",
    "ax.set_ylabel(r'$\\rho$')\n",
    "\n",
    "mask = np.logical_or(Z == Phases.GENERALIZATION, Z == Phases.MEM_FAIL_LEARN_TEACHER).astype(int)\n",
    "\n",
    "XX, YY = np.meshgrid(X, Y)\n",
    "\n",
    "ax.set_title('phenomenology')\n",
    "\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "\n",
    "a = df_orig[df_orig.data_input_dim.isin([200,1600])]\n",
    "a = a[np.logical_or(a.student_train_frac == 0.8,np.isclose(a.student_train_frac,0.80306122))]\n",
    "a = a[a.student_shuffle_input_intra_class == False]\n",
    "\n",
    "metrics = [ 'teacher_train_acc', 'student_train_acc', 'student_test_acc',  ]\n",
    "labels = ['teacher memorization acc', 'student acc on train\\n memorization set', 'student acc on test\\n memorization set',  ]\n",
    "\n",
    "for d, g in a.groupby('data_input_dim'):\n",
    "\n",
    "    grouped = g.groupby('data_num_samples_per_class')\n",
    "\n",
    "    mean_df = grouped.mean(numeric_only=True).reset_index()\n",
    "    sem_df = grouped.sem(numeric_only=True).reset_index()\n",
    "\n",
    "    x = mean_df['data_num_samples_per_class'] * 2 / d\n",
    "\n",
    "    for ax, metric, label in zip(axes, metrics, labels):\n",
    "        y = mean_df[metric]\n",
    "        yerr = sem_df[metric]\n",
    "        if d > 1000:\n",
    "            ax.errorbar(x[::4], y[::4], yerr=yerr[::4], capsize=3, label=d, c=metric_styles[metric]['color'])\n",
    "        else:\n",
    "            ax.errorbar(x, y, yerr=yerr, capsize=3, label=d, c='grey')\n",
    "        ax.set_xlabel(r'$\\alpha$')\n",
    "        ax.set_title(metric_styles[metric]['label'])\n",
    "\n",
    "        if 'mse' in metric:\n",
    "            ax.set_yscale('log')\n",
    "        else:\n",
    "            ax.set_ylim(0.5,1.05)\n",
    "\n",
    "for ax in axes[:3]:\n",
    "    ax.legend(title='$d$')\n",
    "\n",
    "axes[0].axvline(alpha_t_label, color=alpha_t_label_color, linestyle='--')\n",
    "axes[1].axvline(alpha_s_label, color=alpha_s_label_color, linestyle='--')\n",
    "axes[2].axvline(alpha_s_id,color=alpha_s_id_color, linestyle='--',)\n",
    "\n",
    "axes[0].set_ylabel('accuracy')\n",
    "    \n",
    "for ax, name in zip(axes,[\"A.1\",\"A.2\",\"A.3\",\"B\"]):\n",
    "    ax.text(0.05, 1.12 if \"C\" not in name else 1.22, f'({name})', transform=ax.transAxes, fontsize=12, va='top', ha='right',fontweight='bold')\n",
    "\n",
    "\n",
    "axes[-1].plot([],[],label=alpha_t_label_name, color=alpha_t_label_color, linestyle='--')\n",
    "\n",
    "axes[3].axvline(alpha_s_label, color=alpha_s_label_color, linestyle='--', label=alpha_s_label_name)\n",
    "axes[-1].plot([],[],label=alpha_s_id_name, color=alpha_s_id_color, linestyle='--')\n",
    "axes[-1].legend()\n",
    "y = np.linspace(1,2, 1000)\n",
    "y = y[y != 1]  # Remove y=1 to avoid division by zero\n",
    "f_y = 1 / y\n",
    "axes[-1].plot(y, f_y, color=alpha_s_id_color, linestyle='--', label=r'$\\frac{1}{\\rho}$')\n",
    "\n",
    "axes[-1].set_ylim(0.05,0.95)\n",
    "for ax in axes:\n",
    "    ax.set_xlim(x_min,x_max)\n",
    "plt.tight_layout()\n",
    "#plt.title('Knowledge Distillation for Logistic Regression')\n",
    "plt.savefig(FIGURE_DIR / 'logistic_regression_phase_diagram_c=2.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
